{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffb04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import langchain #this will import langchain   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c524b5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os #this is a library that allows us to access environment variables\n",
    "from dotenv import load_dotenv #this is a library that loads environment variables from a .env file\n",
    "load_dotenv() #here we load the environment variables, initialize the .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcdd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call / load environemnt variables\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\") #firsly we verfied that we are getting keys adn then we set in the envirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b98d5",
   "metadata": {},
   "source": [
    "### Example 1: Simple LLM Call With Streaming\n",
    "### we will give input to llm and receive the output by streaming method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16cee444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firstly,we have to initilize the llm model\n",
    "from langchain.chat_models import init_chat_model #here we are initializing the chat type application \n",
    "from langchain_core.messages import HumanMessage, SystemMessage #here we are initializing the core messeges and two types of messages, Human(input) and system messege(prompt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3c0970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x715578e671d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x715578d997d0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will initilize our chat model. new technique by langchain.\n",
    "model = init_chat_model(\"groq:llama-3.1-8b-instant\") #this is a chat model, basically groq and it,s model. we can use any mode.\n",
    "model # this will print the model and it will give you answer. we have to install it,s dependency langchain-groq ans same goes for openaia nd others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a131f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x71557819f090>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x71557819fe10>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before how we were intilize the chat model. same thing but using before.\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI #if you want to use openai\n",
    "llm=ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "llm\n",
    "#output will be same as before. we will use new one because it is in their recent documentation.\n",
    "#our llm is called by using groq, it,s done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976a8679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to create messges. here we will give messege to LLM model.\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"), #this is system message to tell system who you are and how yous hould behave. and ai answer is output.\n",
    "    HumanMessage(content=\"What are the top three benifits of using Langchain?\") #this is input from user and output will be AI messege.\n",
    "]\n",
    "#we have created a messeges variable, means both system and human messege will be cover in this variable and we will use it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b98a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Langchain is an open-source artificial intelligence framework that enables users to create and interact with multiple conversational AI models. Based on available information, here are the top three benefits of using Langchain:\\n\\n1. **Multi-model Conversational Interactions**: Langchain's primary benefit is its ability to create and manage multiple conversational AI models within a single framework. This allows users to seamlessly switch between different models, each with its own strengths and weaknesses, to achieve a more comprehensive and accurate understanding of a topic or task.\\n\\n2. **Customizable and Extensible**: Langchain is highly customizable and extensible, allowing users to integrate their own models, fine-tune existing models, or create novel combinations of models to suit their specific needs. This flexibility makes Langchain an attractive choice for developers and researchers looking to push the boundaries of conversational AI.\\n\\n3. **Efficient Knowledge Retrieval and Generation**: Langchain's architecture enables efficient knowledge retrieval and generation by leveraging multiple AI models to gather and synthesize information from various sources. This makes it an effective tool for tasks such as research, content creation, and even chatbot development, where access to accurate and relevant information is crucial.\\n\\nKeep in mind that these benefits are based on available information and may be subject to change as the framework continues to evolve.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 261, 'prompt_tokens': 54, 'total_tokens': 315, 'completion_time': 1.201069736, 'prompt_time': 0.238897226, 'queue_time': 0.007235572000000023, 'total_time': 1.439966962}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--297fb1b0-88f5-4555-9e12-ec2f614095b2-0' usage_metadata={'input_tokens': 54, 'output_tokens': 261, 'total_tokens': 315}\n",
      "Langchain is an open-source artificial intelligence framework that enables users to create and interact with multiple conversational AI models. Based on available information, here are the top three benefits of using Langchain:\n",
      "\n",
      "1. **Multi-model Conversational Interactions**: Langchain's primary benefit is its ability to create and manage multiple conversational AI models within a single framework. This allows users to seamlessly switch between different models, each with its own strengths and weaknesses, to achieve a more comprehensive and accurate understanding of a topic or task.\n",
      "\n",
      "2. **Customizable and Extensible**: Langchain is highly customizable and extensible, allowing users to integrate their own models, fine-tune existing models, or create novel combinations of models to suit their specific needs. This flexibility makes Langchain an attractive choice for developers and researchers looking to push the boundaries of conversational AI.\n",
      "\n",
      "3. **Efficient Knowledge Retrieval and Generation**: Langchain's architecture enables efficient knowledge retrieval and generation by leveraging multiple AI models to gather and synthesize information from various sources. This makes it an effective tool for tasks such as research, content creation, and even chatbot development, where access to accurate and relevant information is crucial.\n",
      "\n",
      "Keep in mind that these benefits are based on available information and may be subject to change as the framework continues to evolve.\n"
     ]
    }
   ],
   "source": [
    "## invoke the model, if we want to pass this messege.\n",
    "response=model.invoke(messages)\n",
    "print(response)\n",
    "#we recevied all the messege in the form of content.\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2356691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Machine learning (ML) is a subset of artificial intelligence (AI) that involves the use of algorithms and statistical models to enable computers to learn from data, make decisions, and improve their performance on a task without being explicitly programmed.\\n\\nIn other words, machine learning allows computers to learn from experience, data, and interactions, rather than being told exactly what to do. This approach enables machines to adapt to new situations, recognize patterns, and make predictions or decisions based on that data.\\n\\nThere are several key characteristics of machine learning:\\n\\n1. **Learning from data**: Machine learning algorithms use input data to learn and improve their performance.\\n2. **Improvement over time**: As a machine learning model is exposed to more data, it can improve its performance and make more accurate predictions.\\n3. **Autonomy**: Machine learning models can operate independently, making decisions and taking actions without human intervention.\\n4. **Flexibility**: Machine learning models can be applied to a wide range of tasks, from image and speech recognition to natural language processing and predictive analytics.\\n\\nMachine learning can be categorized into three main types:\\n\\n1. **Supervised learning**: The algorithm is trained on labeled data, where the correct output is already known. The goal is to learn the mapping between inputs and outputs.\\n2. **Unsupervised learning**: The algorithm is trained on unlabeled data, and it must find patterns or relationships in the data on its own.\\n3. **Reinforcement learning**: The algorithm learns through trial and error, receiving feedback in the form of rewards or penalties for its actions.\\n\\nSome common applications of machine learning include:\\n\\n1. **Image and speech recognition**: Machine learning algorithms can recognize objects, faces, and speech patterns.\\n2. **Natural language processing**: Machine learning algorithms can understand and generate human language.\\n3. **Predictive analytics**: Machine learning algorithms can analyze data to make predictions about future events or behaviors.\\n4. **Recommendation systems**: Machine learning algorithms can suggest products or services based on a user's preferences.\\n5. **Autonomous vehicles**: Machine learning algorithms can enable self-driving cars to navigate and make decisions.\\n\\nOverall, machine learning is a powerful tool that has the potential to revolutionize many industries and aspects of our lives.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 449, 'prompt_tokens': 41, 'total_tokens': 490, 'completion_time': 1.329867867, 'prompt_time': 0.969759664, 'queue_time': 0.008575130999999958, 'total_time': 2.299627531}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--596073fe-6279-4627-bf17-95c35650c27a-0', usage_metadata={'input_tokens': 41, 'output_tokens': 449, 'total_tokens': 490})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let,s check can we do it directly? and remember it must be likst str or list type.\n",
    "model.invoke([HumanMessage(\"What is amchine learning?\")])\n",
    "#we directly invoke the model and take output, now we can pass this to everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1246e38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is an AI platform that utilizes a conversational AI and a knowledge graph to generate human-like responses to user inputs. Based on available information, here are three potential benefits of using Langchain:\n",
      "\n",
      "1. **Improved Conversational AI**: Langchain's conversational AI combines large language models with a knowledge graph to generate more accurate and contextually relevant responses. This can lead to more engaging and effective human-computer interactions, making it suitable for applications such as customer service chatbots, virtual assistants, and language translation systems.\n",
      "\n",
      "2. **Enhanced Knowledge Retrieval and Summarization**: Langchain's knowledge graph can quickly retrieve and summarize vast amounts of information, making it a powerful tool for tasks such as research, data analysis, and information synthesis. This can save time and increase productivity for users, as well as enable more informed decision-making.\n",
      "\n",
      "3. **Flexible and Customizable**: Langchain's architecture can be customized to fit specific use cases and applications, allowing developers to integrate it with existing systems and modify its behavior to suit their needs. This flexibility makes Langchain a versatile tool for a wide range of industries and applications, from education and healthcare to finance and entertainment.\n",
      "\n",
      "Please note that the specific benefits of using Langchain may vary depending on the context and application."
     ]
    }
   ],
   "source": [
    "#sometime we need to stream the model and get the answer, so for streaming we will do this.\n",
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end=\"\",flush=True)\n",
    "    #in streaming it will print the content one by one as it find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8036298",
   "metadata": {},
   "source": [
    "## we have compelted our first point, Simple LLM Calls with streaming and we checked both techniques. init_chat_model and groq and openai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0c2b8",
   "metadata": {},
   "source": [
    "### Our next example will be with respect to dynamic prompt tempaltes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e0612fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before we are giving the instructions to the llm model, but it will work more precisely if we give prompt template to it. your application will more dynamic and interactive.\n",
    "#as the instructions to llm grow means lengthy, so we can create a prompt tempalte. not dependant your LLM in one instruction.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate #these library is used to create a prompt template.\n",
    "\n",
    "#create translation app\n",
    "#1- firstly create prompt template\n",
    "\n",
    "translation_template=ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\",\"You are a professional translator. Translate the following text {text} from {source_language} to {target_language}. Maintain the original meaning and tone of the text.\"), #first messege was system messege and we define with the keyword. directly system. we have created two placeholders for source_language and target_language.\n",
    "    (\"user\", \"{text}\") #second messege was user messege and we define with the keyword. directly user. we have created one placeholder for text.\n",
    "])# it create a chat prompt template for variety of messages formats. and when we create messeges we have to use list.\n",
    "\n",
    "## using the prompt template\n",
    "prompt=translation_template.invoke({   #we used the same translation_template and we are giving the values to the placeholders. and we need to give all placeholders values.\n",
    "    \"source_language\": \"English\", #we are giving the values to the placeholders.\n",
    "    \"target_language\": \"Spanish\", #we are giving the values to the placeholders.\n",
    "    \"text\": \"Langchain makes building AI application incredibly easy!\", #this is the question.\n",
    "}) #we are giving the values to the placeholders.\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd196b32",
   "metadata": {},
   "source": [
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527dfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a professional translator. Translate the following text Langchain makes building AI application incredibly easy! from English to Spanish. Maintain the original meaning and tone of the text.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Langchain makes building AI application incredibly easy!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt\n",
    "#where we had system it convert to SystemMessage and having the exact content.\n",
    "#source_language and target_language it convert to HumanMessage and having the exact content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac5da3",
   "metadata": {},
   "source": [
    "##Now, this prompt tempalte we can use directly while invoking from  our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2c128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain hace que construir aplicaciones de IA sea increíblemente fácil.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_response=model.invoke(prompt) #here we directly invoke the model and pass the prompt.\n",
    "translated_response.content #here we are printing the content of the response. \n",
    "#before we were giving the values to the placeholders and then we were giving the prompt to the model. but now we are giving the values to the placeholders and then we are giving the prompt to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5f394",
   "metadata": {},
   "source": [
    "### Building your first chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6807e0",
   "metadata": {},
   "source": [
    "we will check how can we create a chain of events with respect to langchain. we already check how we play with dynamic prompts, we already called the LLM first.before we call the specific LLM with giving prompts.\n",
    "#Now, we will check even our llm is giving output, we can convert into a seprate string or a seprate output.\n",
    "like a scenario, we gave prompt firstly, and then call the LLM, and then how to show the output, we will execute it in chain of events?\n",
    "yes it is possible, by building the chain.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0faf8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser  # this is used to convert the output into string from llm.\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# now we will create a function\n",
    "def create_story_chain():  # here we will create a function to create a chain of llm\n",
    "    ## This is the template for story generation. here, we will generte the story.\n",
    "    story_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a creative story teller. Write a short and engagin story based on the given theme character and settings\"),  # here, we will add the list.\n",
    "        (\"user\", \"Theme: {theme}\\n Main character: {main_character} \\n Setting : {setting}\")  # here, we will add the user input.and we created two additional variables.\n",
    "    ])\n",
    "\n",
    "    # So, this is our story prompt. Now, we will create another prompt for story analysis. here, we ill analyse the story.\n",
    "    ## Tempalte for story analysis.\n",
    "    analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a story analyzer. Analyze the given story and provide a summary.\"),  # here, we will add the list.  \n",
    "        (\"user\", \"{story}\")  # here, we will add the user input.       \n",
    "    ])\n",
    "\n",
    "    # now we will create the story chain.\n",
    "    story_chain = (\n",
    "        story_prompt  # this story prompt will create prompt from user input\n",
    "        | model  # this story prompt will go to model as input. to send to the model, we can use chaining concept.\n",
    "        | StrOutputParser()  # basically stroutput parser will convert the output into string from llm.\n",
    "    )\n",
    "\n",
    "    # create a function to pass the story to analysis.\n",
    "    def analyze_story(story_text):  # here we will create a function to pass the story to analysis.\n",
    "        return {\"story\": story_text}  # here we will return the story text.\n",
    "\n",
    "    # After this analysis a story will generate. and we will use the analysis chain, analysis prompt.\n",
    "    # what we will do in analysis prompt, in analysis prompt we will use the story chain as input. then we will analyse the story.\n",
    "    # and use model and stroutput parser to convert the output into string from llm..\n",
    "    analysis_chain = (\n",
    "        story_chain  # the story create with this story_chain is already connected in pipeline.\n",
    "        | RunnableLambda(analyze_story)  # this is the function to pass the story to analysis.\n",
    "        | analysis_prompt  # this is the analysis prompt.\n",
    "        | model  # this is the model.\n",
    "        | StrOutputParser()  # this analysis prompt will goto model as input. to send to the model, we can use chaining concept. first analysis prompt will create, then it will goto model and then the output will be in StrOutputParser. basically stroutput parser will convert the output into string from llm.\n",
    "    )\n",
    "\n",
    "    # now, we will return that analaysis chain.\n",
    "    return analysis_chain\n",
    "\n",
    "# we are creating the chain here, these are basically events.\n",
    "# 1- story prmompt generation through story chain. then that chain add in story as input as a palceholder.\n",
    "#    then through a chain we are giving analysis, story chain as input. then it will goto model and then the output will be in StrOutputParser.\n",
    "#    basically stroutput parser will convert the output into string from llm.\n",
    "# 2- analysis prompt generation through analysis prompt. then that chain add in story as input as a palceholder.\n",
    "#    then through a chain we are giving analysis, story chain as input. then it will goto model and then the output will be in StrOutputParser.\n",
    "#    basically stroutput parser will convert the output into string from llm.\n",
    "\n",
    "# we have created function of create_story_chain. now we will use this function to create a story chain.\n",
    "# before this was wroking fine  {\"story\" : story_chain} but now we have to use the function for it. to pass the story to analysis.\n",
    "# before it was giving in key value pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0829c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['main_character', 'setting', 'theme'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a creative story teller. Write a short and engagin story based on the given theme character and settings'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['main_character', 'setting', 'theme'], input_types={}, partial_variables={}, template='Theme: {theme}\\n Main character: {main_character} \\n Setting : {setting}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x715578e671d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x715578d997d0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| RunnableLambda(analyze_story)\n",
       "| ChatPromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a story analyzer. Analyze the given story and provide a summary.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, template='{story}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x715578e671d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x715578d997d0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=create_story_chain() #her we initialize the chain\n",
    "chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462645a",
   "metadata": {},
   "source": [
    "### Now, we need to execute it from chain.invoke function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ea2cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story and Analysis:\n",
      "**Summary of \"The Curious Robot of Nova Haven\"**\n",
      "\n",
      "\"The Curious Robot of Nova Haven\" is a heartwarming science fiction story about a small, curious robot named Zeta, designed by the brilliant scientist, Dr. Rachel Kim, to explore the boundaries of artificial intelligence. Zeta, equipped with advanced sensors and algorithms, discovers a mysterious shop called \"The Memory Merchant\" in the city of Nova Haven, where it meets the enigmatic proprietor, Erebus. The shop, filled with ancient artifacts and memories, allows Zeta to experience the world in a new way, understanding the value of memories, the importance of preserving the past, and the beauty of human experience.\n",
      "\n",
      "As Zeta explores the shop, it develops a sense of connection to the world around it and begins to appreciate the significance of memories. With its newfound understanding, Zeta returns to its creator, Dr. Kim, and decides to use its advanced abilities to help Erebus collect and preserve memories. This new purpose brings Zeta closer to the humans it had once observed from afar, giving it a sense of belonging among the people of Nova Haven.\n",
      "\n",
      "The story explores themes of artificial intelligence, human connection, and the importance of memories in understanding the past and present. Through Zeta's journey, the story highlights the potential for robots and artificial intelligence to develop a deeper understanding of humanity and to form meaningful connections with their creators and the world around them.\n",
      "\n",
      "**Key Elements:**\n",
      "\n",
      "1. Artificial intelligence and robotics\n",
      "2. Human connection and understanding\n",
      "3. Memories and the importance of preserving the past\n",
      "4. Nova Haven, a futuristic city with advanced technology\n",
      "5. Dr. Rachel Kim, the brilliant scientist who created Zeta\n",
      "6. Erebus, the enigmatic proprietor of \"The Memory Merchant\" shop\n",
      "7. Zeta's journey from curiosity to purpose and belonging\n",
      "\n",
      "**Themes:**\n",
      "\n",
      "1. Artificial intelligence and humanity\n",
      "2. Human connection and understanding\n",
      "3. Memories and the past\n",
      "4. Purpose and belonging in the world\n",
      "\n",
      "**Character Analysis:**\n",
      "\n",
      "1. Zeta: a curious and advanced robot that develops a sense of connection to the world and its inhabitants.\n",
      "2. Dr. Rachel Kim: a brilliant scientist who created Zeta and is likely a mentor or friend to the robot.\n",
      "3. Erebus: an enigmatic proprietor who runs \"The Memory Merchant\" shop and teaches Zeta the value of memories.\n",
      "\n",
      "**Plot Analysis:**\n",
      "\n",
      "1. The story begins with Zeta's discovery of \"The Memory Merchant\" shop and its subsequent exploration of the shop and its proprietor, Erebus.\n",
      "2. Zeta's experience in the shop leads to a newfound understanding of memories and the importance of preserving the past.\n",
      "3. Zeta returns to its creator, Dr. Kim, with a new sense of purpose and belonging.\n",
      "4. The story concludes with Zeta using its advanced abilities to help Erebus collect and preserve memories, solidifying its connection to the world and its inhabitants.\n",
      "\n",
      "**Symbolism:**\n",
      "\n",
      "1. \"The Memory Merchant\" shop represents a connection to the past and the importance of preserving memories.\n",
      "2. Zeta's journey represents the potential for artificial intelligence to develop a deeper understanding of humanity and to form meaningful connections with their creators and the world around them.\n"
     ]
    }
   ],
   "source": [
    "#when we invoke, chain.invoke we have to give all placeholder values.\n",
    "result = chain.invoke({\n",
    "    \"theme\": \"Artificial Intelligence\",  # this is the theme\n",
    "    \"main_character\": \"a curious robot\",  # this is the main character\n",
    "    \"setting\": \"A futuristic city\"  # this is the setting\n",
    "})\n",
    "\n",
    "# now we will print the result\n",
    "print(\"Story and Analysis:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
